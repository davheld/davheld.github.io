<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
    <link rel="stylesheet" type="text/css" href="david.css">
        <head>
            <style>
            #new{
                background-color: rgb(180,0,0);
                color: rgb(255,255,255);
            }
            </style>
        </head>
        
        <script type="text/javascript">
            
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-38307145-1']);
            _gaq.push(['_setDomainName', 'stanford.edu']);
            _gaq.push(['_trackPageview']);
            
            (function() {
             var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
             ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
             var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
             })();
             
            </script>
        
        <!--script type="text/javascript" async="" src="./DavidHeld_files/ga.js"></script><script type="text/javascript"-->
        
        <script type="text/javascript">
            function copy(dest, source) {
                if(dest.source == source) {
                    dest.innerHTML = "";
                    dest.source = null;
                } else {
                    dest.innerHTML = source.innerHTML;
                    dest.source = source;
                }
                dest.blur();
            }
        </script>
        <title>David Held</title>
</head>

<body>
    <div id="outer">
        
        <div class="top">
            <div class="left">
                <img src="./DavidHeld_files/DavidHeld3.png" id="name">
            </div>
            <div class="right">
            </div>
        </div>
        
        <div class="bottom">
            <div class="left">
                <img src="./DavidHeld_files/bg-grey.png" id="gradient">
            </div>
            <div class="right">
                <div style="float:left; width:100%">
                    
                    <!--BEGIN CONTENT-->
                    
                    <!-- About -->
                    <table><tr>
                        <td>
                            <div class="heading"> About Me </div>
                            <p>
                            I am an assistant professor at CMU in the <a href="http://ri.cmu.edu/">Robotics Institute</a>.
                            Prior to my appointment at CMU, I worked as a post-doc at UC Berkeley with <a href="http://www.cs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> on deep reinforcement learning for object manipulation.
                            I completed my Ph.D. in computer science at Stanford working with <a href="http://robots.stanford.edu/">Sebastian Thrun</a> and <a href="http://cvgl.stanford.edu/index.html">Silvio Savarese</a> on perception for self-driving cars.  I also have a B.S. and M.S.
                            in mechanical engineering from MIT.
                            <!--Prior to that I did research at the <a href="http://www.weizmann.ac.il/">Weizmann Institute</a>, worked in industry as a software developer, and received a B.S. and M.S.
                            in mechanical engineering at MIT.-->
                            </p>
                            
                            <p>
                            You can also download my <a href="DavidHeld_files/DavidHeld_CV.pdf">CV</a>.
                            </p>
                            <div class="heading"> My Group </div><br>
                            I have the great fortune to be working with a tremendous group of Ph.D. students:
                            <br><br>
                            Brian Okorn (RI), co-advised with Martial Hebert<br>
                            Xingyu Lin (RI)<br>
                            Siddharth Ancha (MLD)<br>
                            Thomas Weng (RI)<br>
                            Wenxuan Zhou (RI)<br>
                            <br>
                            <div class="heading"> Joining my Group </div><br>
                            If you are interested in coming to CMU to join my group as a Ph.D. student, there is no need to email me.  Just apply to CMU's Ph.D. program!  You should apply to either the <a href="https://www.ri.cmu.edu/education/academic-programs/phd/">Robotics Institute Ph.D. program</a> or the <a href="http://www.ml.cmu.edu/academics/ml-phd.html">Machine Learning Ph.D. program</a> and mention my name in your research statement.  After you get accepted, you should contact me to discuss the possibility of working in my group.
                            <br><br>
                            
                            <div class="heading"> Teaching </div> <br>
                            Spring 2018: 16-831: Statistical Techniques in Robotics<br>
                            <a href="https://sites.google.com/view/16-881-cmu/past-courses/2019-spring-home?authuser=0">Spring 2019: 16-881: Seminar: Deep Reinforcement Learning for Robotics</a><br>
                            Fall 2019: 16-831: Statistical Techniques in Robotics<br>
                            <a href="https://sites.google.com/view/16-881-cmu/home?authuser=0">Spring 2020: 16-881: Seminar: Deep Reinforcement Learning for Robotics</a><br>
                            <br>

                            
                            <!--<b id="new">NEW</b>  I am excited to announce that I will be starting as an assistant professor at CMU in the <a href="http://ri.cmu.edu/">Robotics Institute</a> in Fall 2017! <br>
                            <p>-->
                            <div class="heading"> Research Interests </div>
                            <p>
                            <!--I am generally interested in robotics, vision, and machine learning.-->
                            <!--Berkeley Artificial Intelligence Research laboratory-->
                            
                            <!--I am generally interested in developing models for robot perception and control, using techniques from robotics, computer vision, and machine learning.
                             My latest research has been on modeling how the appearance of an object can change over time.  I have applied this idea to the problems of segmentation, tracking, velocity estimation, and object recognition for <a href="http://driving.stanford.edu/index.html">autonomous driving</a>, and I have shown how, by modeling these appearance changes, we can significantly improve a robot's perception capabilities.-->
                            <p>My research lies at the intersection of robotics, machine learning, and computer vision.
                            </p>
                            <p>I am interested in developing methods for robotic perception and control that can allow robots to operate in the messy, cluttered environments of our daily lives.  My approach is to design new deep learning / machine learning algorithms to understand environmental changes: how dynamic objects in the environment can move and how to affect the environment to achieve a desired task.
                            </p>
                            <p>I have applied this idea of learning to understand environmental changes to
                            improve a robot's capabilities in two domains: object manipulation and autonomous driving.  I am currently working on
                            learning to control indoor robots for various object manipulation tasks, dealing with questions about multi-task learning, robust learning, simulation to real-world transfer, and safety. Within <a href="http://driving.stanford.edu/index.html">autonomous driving</a>, I have shown how, by modeling object
                            appearance changes, we can improve a robot's capabilities for every
                            part of the robot perception pipeline: segmentation, tracking,
                            velocity estimation, and object recognition.  By teaching
                            robots to understand and affect environmental changes, I hope to open the door
                            to many new robotics applications, such as robots for our homes, assisted living
                            facilities, schools, hospitals, or disaster relief areas.
                            </p>
                            
                            <!--The following video provides a decent overview of my current research and some of my current interests:<br><br>
                            <iframe width="569" height="320" src="https://www.youtube.com/embed/FTaQqpx_xT0" frameborder="0" allowfullscreen></iframe>
                            <br-->
                            <p>
                            
                            <!-- Awards -->
                            <!--div class="heading"> Awards </div>
                            <ul>
                                <li><!--Christofer Stephenson Memorial Award for Graduate Research (Best Master's Thesis of 2012 in Stanford's Computer Science Department). -->
                                    <!--Best Master's Thesis of 2012 in Stanford's Computer Science Department <br>
                                    M.S. Thesis: "Autonomous Driving: Car Detection, Tracking, and Street Sign Detection,"
                                    <br>
                                    co-advised by
                                    <a href="http://boole.stanford.edu/pratt.html">Vaughan Pratt</a>
                                    and <a href="http://robots.stanford.edu/">Sebastian Thrun</a>.</li>
                            </ul-->
                            
                            
                            <!-- Preprints -->
                            <div class="heading"> Preprints </div>
                            
                            <table cellpadding="10" text-align="left">
                                
                                <!--tr><td><li class="year"> 2017</li></td></tr-->
                                
                                <tr>
                                    <td class="pubimg" width="250" height="100">
                                        <img src="pics/peiyuncvpr2020/splash.jpg" img width="250">
                                    </td>
                                    <td><a href="https://arxiv.org/abs/1912.04986">What You See is What You Get: Exploiting Visibility for 3D Object Detection</a><br>
                                        <div class="pubauthor">
                                            Peiyun Hu, Jason Ziglar, David Held, Deva Ramanan<br>
                                        </div>
                                        <!--div class="pubjournal">International Conference of Robotics and Automation (ICRA), 2019</div-->
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                <tr>
                                    <td class="pubimg" width="250" height="100">
                                        <img src="DavidHeld_files/indicator2019.png" img width="250">
                                    </td>
                                    <td><a href="https://arxiv.org/abs/1905.07866">Reinforcement Learning without Ground-Truth State</a><br>
                                        <div class="pubauthor">
                                            Xingyu Lin, Harjatin Singh Baweja, David Held<br>
                                        </div>
                                        <!--div class="pubjournal">International Conference of Robotics and Automation (ICRA), 2019</div-->
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                <tr>
                                    <td class="pubimg" width="250" height="100">
                                        <img src="DavidHeld_files/itn2018.png" width="250" height="100">
                                    </td>
                                    <td><a href="https://arxiv.org/abs/1811.11209">Iterative Transformer Network for 3D Point Cloud</a><br>
                                        <div class="pubauthor">
                                            Wentao Yuan, David Held, Christoph Mertz, Martial Hebert<br>
                                        </div>
                                        <!--div class="pubjournal">International Conference of Robotics and Automation (ICRA), 2019</div-->
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                            </table>
                            <br><br>

                            
                            <!-- Publications -->
                            <div class="heading"> Publications </div>
                            
                            <table cellpadding="10" width="100%" >
                                
                                <!--tr><td><li class="year"> 2017</li></td></tr-->
                                <tr>
                                    <td class="pubimg" width="250" height="100">
                                        <img src="pics/peiyunral2020/output.jpg" img width="250">
                                    </td>
                                    <td><a href="https://arxiv.org/abs/1912.04976">Learning to Optimally Segment Point Clouds</a><br>
                                        <div class="pubauthor">
                                            Peiyun Hu, David Held*, Deva Ramanan*<br>
                                        </div>
                                        <div class="pubjournal">IEEE Robotics and Automation Letters (RA-L), 2020</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                <tr>
                                    <td class="pubimg">
                                        <img src="pics/CoRL2019.png" width="250">
                                    </td>
                                    <td><a href="https://arxiv.org/abs/1912.12270">Combining Deep Learning and Verification for Precise Object Instance Detection<a/><br>
                                        <div class="pubauthor">
                                            Siddharth Ancha*, Junyu Nan*, David Held</div>
                                        <div class="pubjournal">Conference on Robot Learning (CoRL), 2019</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                <tr>
                                    <td class="pubimg">
                                        <img src="pics/2019_olaux.png" width="250" height="100">
                                            </td>
                                    <td><a href="https://papers.nips.cc/paper/8724-adaptive-auxiliary-task-weighting-for-reinforcement-learning">Adaptive Auxiliary Task Weighting for Reinforcement Learning</a><br>
                                        <div class="pubauthor">
                                            Xingyu Lin*, Harjatin Baweja*, George Kantor, David Held</div>
                                        <div class="pubjournal">Neural Information Processing Systems (NeurIPS), 2019</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/icra19.png" width="250" height="100">
                                    </td>
                                    <td><a href="https://arxiv.org/abs/1903.06309">Adaptive Variance for Changing Sparse-Reward Environments</a><br>
                                        <div class="pubauthor">
                                            Xingyu Lin, Pengsheng Guo, Carlos Florensa, David Held</div>
                                        <div class="pubjournal">International Conference of Robotics and Automation (ICRA), 2019</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/pcn.png" width="250" height="100">
                                            </td>
                                    <td><a href="http://arxiv.org/abs/1808.00671">PCN: Point Completion Network - <award>Best Paper Honorable Mention</award></a><br>
                                        <div class="pubauthor">
                                            Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, Martial Hebert</div>
                                        <div class="pubjournal">International Conference on 3D Vision (3DV), 2018</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                
                                
                                <!--ICML 2018-->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/ant-maze.png" width="250" height="100">
                                            </td>
                                    <td><a href="https://arxiv.org/abs/1705.06366">Automatic Goal Generation for Reinforcement Learning Agents</a><br>
                                        <div class="pubauthor">
                                            Carlos Florensa*, David Held*, Xinyang Geng*, Pieter Abbeel</div>
                                        <div class="pubjournal">International Conference on Machine Learning (ICML), 2018</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/enabling_robots_RSS_2017.png" width="250">
                                            </td>
                                    <td><a href="https://rdcu.be/ZM50">Enabling Robots to Communicate their Objectives</a><br>
                                        <div class="pubauthor">
                                            Sandy Han Huang, David Held, Pieter Abbeel, Anca D. Dragan</div>
                                        <div class="pubjournal">Autonomous Robotics (AURO), 2018</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>


                                <!--tr><td><li class="year"> 2017</li></td></tr-->
                                
                                <!-- CoRL 2017-->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/reverse_curr2.png" width="250" height="100">
                                            </td>
                                    <td><a href="https://arxiv.org/abs/1707.05300">Reverse Curriculum Generation for Reinforcement Learning</a><br>
                                        <div class="pubauthor">Carlos Florensa, David Held, Markus Wulfmeier, Pieter Abbeel</div>
                                        <div class="pubjournal">Conference on Robot Learning (CoRL), 2017</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                <!--IROS 2017-->
                                <tr>
                                    <td>
                                        <img src="DavidHeld_files/transfer.png" width="250">
                                            </td>
                                    <td><a href="DavidHeld_files/IROS___RL_pushing.pdf">Policy  Transfer via Modularity</a><br>
                                        <div class="pubauthor">
                                            Ignasi Clavera*, David Held*, Pieter Abbeel</div>
                                        <div class="pubjournal">International Conference on Intelligent Robots and Systems (IROS), 2017</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                <!--ICML 2017-->
                                <tr>
                                    <td>
                                        <img src="DavidHeld_files/humanoid.png" width="250">
                                            </td>
                                    <td><a href="https://arxiv.org/abs/1705.10528">Constrained Policy Optimization</a><br>
                                        <div class="pubauthor">
                                            Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel</div>
                                        <div class="pubjournal">International Conference on Machine Learning (ICML), 2017</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                <!--RSS 2017-->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/enabling_robots_RSS_2017.png" width="250">
                                            </td>
                                    <td><a href="https://arxiv.org/abs/1702.03465">Enabling Robots to Communicate their Objectives</a><br>
                                        <div class="pubauthor">
                                            Sandy H. Huang, David Held, Pieter Abbeel, Anca D. Dragan</div>
                                        <div class="pubjournal">Robotics: Science and Systems (RSS), 2017</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                <!--ICRA 2017-->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/safe_transfer3.png" width="250">
                                            </td>
                                    <td><a href="https://arxiv.org/abs/1705.05394">Probabilistically Safe Policy Transfer</a><br>
                                        <div class="pubauthor">
                                            David Held, Zoe McCarthy, Michael Zhang, Fred Shentu, Pieter Abbeel</div>
                                        <div class="pubjournal">International Conference on Robotics and Automation (ICRA), 2017</div>
                                        <!--div style="font-size:small">
                                         <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                         <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                         <a href="GOTURN/supplement.pdf">[Supplement]</a></div-->
                                        <br>
                                    </td>
                                </tr>
                                
                                
                                <!--tr><td><li class="year"> 2016</li></td></tr-->
                                
                                <!--ul-->
                                <!-- ------------------------------------------------------------------------
                                 2016
                                 --------------------------------------------------------------------- -->
                                <!--li class="pubitem"-->
                                <!--table border="0" cellpadding="0" cellspacing="0" cellpadding="100" width="800" bgcolor="#FFFFFF"-->
                                
                                
                                
                                
                                <!--ECCV 2016-->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/pull7f-web_d.png" width="250">
                                            </td>
                                    <td><a href="GOTURN/GOTURN.pdf">Learning to Track at 100 FPS with Deep Regression Networks</a><br>
                                        <div class="pubauthor">David Held, Sebastian Thrun, Silvio Savarese</div>
                                        <div class="pubjournal">European Conference on Computer Vision (ECCV), 2016</div>
                                        <div style="font-size:small">
                                            <a href="GOTURN/GOTURN.html">[Project Page]</a>
                                            <a href="GOTURN/GOTURN.pdf">[Full Paper]</a>
                                            <a href="GOTURN/supplement.pdf">[Supplement]</a></div>
                                        <br>
                                    </td>
                                </tr>
                                
                                <!--RSS 2016-->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/pull_fig2.png" width="250">
                                            </td>
                                    <td><a href="segmentation3D/segmentation3D.html">A Probabilistic Framework for Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues</a><br>
                                        <div class="pubauthor">David Held, Devin Guillory, Brice Rebsamen, Sebastian Thrun, Silvio Savarese</div>
                                        <div class="pubjournal">Robotics: Science and Systems (RSS), 2016</div>
                                        <a href="segmentation3D/segmentation3D.html"><div style="font-size:small">[Project Page]</div></a><br>
                                    </td>
                                </tr>
                                
                                
                                <!--ICRA 2016-->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/pull6_flatb.png" width="250">
                                            </td>
                                    
                                    
                                    <td><a href="http://arxiv.org/abs/1507.08286">Robust Single-View Instance Recognition</a><br>
                                        <div class="pubauthor">David Held, Sebastian Thrun, Silvio Savarese</div>
                                        <div class="pubjournal">International Conference on Robotics and Automation (ICRA), 2016</div>
                                        <!--a href="http://arxiv.org/abs/1507.08286">Arxiv version</a-->
                                    </td></tr>
                                
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/tracker_comparison3.png" height="120">
                                            </td>
                                    
                                    
                                    <td>
                                        <a href="DavidHeld_files/ijrr_tracking.pdf">Robust Real-Time Tracking Combining 3D Shape, Color, and Motion </a><br>
                                        <div class="pubauthor">David Held, Jesse Levinson, Sebastian Thrun, Silvio Savarese</div>
                                        <div class="pubjournal">International Journal of Robotics Research (IJRR), 2016</div>
                                        <div class="pubspan"><a href="anytime_tracking.html">[Project Page]</a></div><br>
                                        <!--a href="DavidHeld_files/ijrr_tracking.pdf">[Full paper]</a-->
                                    </td></tr>
                                <!--tr><td><li class="year"> 2015 </li></td></tr-->
                                
                                
                                <!--tr><td><li class="year"> 2014 </li></td></tr-->
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/Pull3.png" height="120">
                                            </td>
                                    
                                    <td class="pubitem">
                                        <div class="pubitem">
                                            <a class="pubtitle" href="papers/rss14_tracking.pdf">
                                                Combining 3D Shape, Color, and Motion for Robust Anytime Tracking</a>
                                            <div>
                                                <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>,
                                                <a class="pubauthor">Jesse Levinson</a>,
                                                <a class="pubauthor" href="http://robots.stanford.edu/">Sebastian Thrun,</a>
                                                <a class="pubauthor" href="http://cvgl.stanford.edu/silvio/">Silvio Savarese</a>
                                            </div>
                                            <div class="pubjournal">
                                                Robotics: Science and Systems
                                                (RSS), 2014
                                            </div>
                                            <!--a href="anytime_tracking.html" fontsize="10">[Project Page]</a-->
                                            
                                            <div id="abs2014rss" style="display:none">
                                                <blockquote>
                                                    Although object tracking has been studied for decades, real-time tracking algorithms often suffer from low accuracy and poor robustness when confronted with difficult, real-world data. We present a tracker that combines 3D shape, color (when available), and motion cues to accurately track moving objects in real-time. Our tracker allocates computational effort based on the shape of the posterior distribution. Starting with a coarse approximation to the posterior, the tracker successively refines this distribution, increasing in tracking accuracy over time. The tracker can thus be run for any amount of time, after which the current approximation to the posterior is returned. Even at a minimum runtime of 0.7 milliseconds, our method outperforms all of the baseline methods of similar speed by at least 10%. If our tracker is allowed to run for longer, the accuracy continues to improve, and it continues to outperform all baseline methods. Our tracker is thus anytime, allowing the speed or accuracy to be optimized based on the needs of the application.
                                                </blockquote>
                                            </div>
                                            
                                            <div id="bib2014rss" style="display:none">
                                                <blockquote>
                                                    <pre>@INPROCEEDINGS{Held-RSS-14,
                                                        AUTHOR    = {David Held AND Jesse Levinson AND Sebastian Thrun AND Silvio Savarese},
                                                        TITLE     = {Combining 3D Shape, Color, and Motion for Robust Anytime Tracking},
                                                        BOOKTITLE = {Proceedings of Robotics: Science and Systems},
                                                        YEAR      = {2014},
                                                        ADDRESS   = {Berkeley, USA},
                                                        MONTH     = {July} } </pre>
                                                </blockquote>
                                            </div>
                                            
                                            <div class="pubspan">
                                                [<a href="javascript:copy(div2014rss, abs2014rss)">abstract</a>]
                                                [<a href="javascript:copy(div2014rss, bib2014rss)">bibtex</a>]
                                                [<a href="anytime_tracking.html">Project Page</a>]
                                            </div>
                                            <div id="div2014rss" class="pubInfo"></div>
                                            <!--/li-->
                                        </div>
                                    </td></tr>
                                
                                <!--tr><td><li class="year"> 2013 </li></td></tr-->
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/3DModels.png" height="120">
                                            </td>
                                    
                                    <td class="pubitem">
                                        <div class="pubitem">
                                            <a class="pubtitle" href="DavidHeld_files/ICRA13_0624_FI.pdf">Precision Tracking with Sparse 3D and Dense Color 2D Data - <award>Best Vision Paper Finalist</award></a>
                                            <div>
                                                <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>,
                                                <a class="pubauthor">Jesse Levinson</a>,
                                                <a class="pubauthor" href="http://robots.stanford.edu/">Sebastian Thrun</a>
                                            </div>
                                            <div class="pubjournal">
                                                International Conference on Robotics and Automation
                                                (ICRA), 2013
                                            </div>
                                            
                                            <div id="abs2013icra" style="display:none">
                                                <blockquote>
                                                    Precision tracking is important for predicting the
                                                    behavior of other cars in autonomous driving. We present a
                                                    novel method to combine laser and camera data to achieve
                                                    accurate velocity estimates of moving vehicles. We combine
                                                    sparse laser points with a high-resolution camera image to
                                                    obtain a dense colored point cloud. We use a color-augmented
                                                    search algorithm to align the dense color point clouds from
                                                    successive time frames for a moving vehicle, thereby obtaining
                                                    a precise estimate of the tracked vehicle’s velocity. Using this
                                                    alignment method, we obtain velocity estimates at a much
                                                    higher accuracy than previous methods. Through pre-ﬁltering,
                                                    we are able to achieve near real time results. We also present an
                                                    online method for real-time use with accuracies close to that of
                                                    the full method. We present a novel approach to quantitatively
                                                    evaluate our velocity estimates by tracking a parked car in
                                                    a local reference frame in which it appears to be moving
                                                    relative to the ego vehicle. We use this evaluation method to
                                                    automatically quantitatively evaluate our tracking performance
                                                    on 466 separate tracked vehicles. Our method obtains a mean
                                                    absolute velocity error of 0.27 m/s and an RMS error of 0.47
                                                    m/s on this test set. We can also qualitatively evaluate our
                                                    method by building color 3D car models from moving vehicles.
                                                    We have thus demonstrated that our method can be used for
                                                    precision car tracking with applications to autonomous driving
                                                    and behavior modeling.
                                                </blockquote>
                                            </div>
                                            
                                            <div id="bib2013icra" style="display:none">
                                                <blockquote>
                                                    <pre>@inproceedings{2013-held-precision,
                                                        title = {Precision Tracking with Sparse 3D and Dense Color 2D Data},
                                                        author = {David Held and Jesse Levinson and Sebastian Thrun},
                                                        booktitle = {ICRA},
                                                        year = {2013} }</pre>
                                                </blockquote>
                                            </div>
                                            
                                            <div class="pubspan">
                                                [<a href="javascript:copy(div2013icra, abs2013icra)">abstract</a>]
                                                [<a href="javascript:copy(div2013icra, bib2013icra)">bibtex</a>]
                                                [<a href="anytime_tracking.html">Project Page</a>]
                                            </div>
                                            <div id="div2013icra" class="pubInfo"></div>
                                            <!--/li-->
                                        </div>
                                    </td></tr>
                                
                                
                                <!-- ------------------------------------------------------------------------
                                 2012
                                 --------------------------------------------------------------------- -->
                                <!--tr><td><li class="year"> 2012 </li></td></tr-->
                                <!--
                                 2012 Car Detection (ICRA)
                                 -->
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/distanceScores.jpg" width="180">
                                            </td>
                                    
                                    
                                    <td class="pubitem"><div class="pubitem">
                                        <a class="pubtitle" href="papers/scaleDetection.pdf">A Probabilistic Framework for Car Detection in Images using Context and Scale</a>
                                        <!-- Official link: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6224722 -->
                                        <div>
                                            <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>,
                                            <a class="pubauthor">Jesse Levinson</a>,
                                            <a class="pubauthor" href="http://robots.stanford.edu/">Sebastian Thrun</a>
                                        </div>
                                        <div class="pubjournal">
                                            International Conference on Robotics and Automation
                                            (ICRA), 2012
                                        </div>
                                        
                                        <div id="abs2010emnlp" style="display:none">
                                            <blockquote>
                                                Detecting cars in real-world images is an important task for autonomous driving, yet it remains unsolved. The system described in this paper takes advantage of context and scale to build a monocular single-frame image-based car detector that significantly outperforms the baseline. The system uses a probabilistic model to combine multiple forms of evidence for both context and scale to locate cars in a real-world image. We also use scale filtering to speed up our algorithm by a factor of 3.3 compared to the baseline. By using a calibrated camera and localization on a road map, we are able to obtain context and scale information from a single image without the use of a 3D laser. The system outperforms the baseline by an absolute 9.4% in overall average precision and 11.7% in average precision for cars smaller than 50 pixels in height, for which context and scale cues are especially important.
                                            </blockquote>
                                        </div>
                                        
                                        <div id="bib2010emnlp" style="display:none">
                                            <blockquote>
                                                <pre>@INPROCEEDINGS{6224722,
                                                    author={Held, D. and Levinson, J. and Thrun, S.},
                                                    booktitle={Robotics and Automation (ICRA), 2012 IEEE International Conference on}, title={A probabilistic framework for car detection in images using context and scale},
                                                    year={2012},
                                                    month={may},
                                                    volume={},
                                                    number={},
                                                    pages={1628 -1634},
                                                    keywords={Cameras;Computational modeling;Context;Context modeling;Detectors;Roads;Training;automobiles;object detection;probability;traffic engineering computing;autonomous driving;car detection;context information;monocular single-frame image-based car detector;probabilistic framework;scale filtering;scale information;},
                                                    doi={10.1109/ICRA.2012.6224722},
                                                    ISSN={1050-4729}
                                                    }</pre>
                                            </blockquote>
                                        </div>
                                        
                                        <div class="pubspan">
                                            [<a href="javascript:copy(div2010emnlp, abs2010emnlp)">abstract</a>]
                                            [<a href="javascript:copy(div2010emnlp, bib2010emnlp)">bibtex</a>]
                                            [<a href="papers/scaleDetection.pdf">Full Paper</a>]
                                        </div>
                                        <div id="div2010emnlp" class="pubInfo"></div>
                                    </div></td></tr>
                                <tr><td><li class="year"> Older Work </li></td></tr>
                                
                                <!--
                                 2012 Octopus (ICRA)
                                 -->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/armforce.png" width="180">
                                            </td>
                                    <td class="pubitem">
                                        <div class="pubitem">
                                            <a class="pubtitle" href="papers/ICRA12_1385_FI.pdf">Characterizing Stiffness of Multi-Segment Flexible Arm Movements</a>
                                            <!-- Official link: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6225070 -->
                                            <div>
                                                <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>,
                                                Yoram Yekutieli,
                                                <a class="pubauthor" href="http://www.wisdom.weizmann.ac.il/~tamar/">Tamar Flash</a>
                                            </div>
                                            <div class="pubjournal">
                                                International Conference on Robotics and Automation
                                                (ICRA), 2012
                                            </div>
                                            
                                            <div id="abs2012octopus" style="display:none">
                                                <blockquote>
                                                    A number of robotic studies have recently turned to biological inspiration in designing control schemes for flexible robots. Examples of such robots include continuous manipulators inspired by the octopus arm. However, the control strategies used by an octopus in moving its arms are still not fully understood. Starting from a dynamic model of an octopus arm and a given set of muscle activations, we develop a simulation technique to characterize the stiffness throughout a motion and at multiple points along the arm. By applying this technique to reaching and bending motions, we gain a number of insights that can help a control engineer design a biologically inspired impedance control scheme for a flexible robot arm. The framework developed is a general one that can be applied to any motion for any dynamic model. We also propose a theoretical analysis to efficiently estimate the stiffness analytically given a set of muscle activations. This analysis can be used to quickly evaluate the stiffness for new static configurations and dynamic movements.
                                                </blockquote>
                                            </div>
                                            
                                            <div id="bib2010octopus" style="display:none">
                                                <blockquote>
                                                    <pre>@INPROCEEDINGS{6225070,
                                                        author={Held, D. and Yekutieli, Y. and Flash, T.},
                                                        booktitle={Robotics and Automation (ICRA), 2012 IEEE International Conference on}, title={Characterizing the stiffness of a multi-segment flexible arm during motion},
                                                        year={2012},
                                                        month={may},
                                                        volume={},
                                                        number={},
                                                        pages={3825 -3832},
                                                        keywords={Computational modeling;Force;Motion segmentation;Muscles;Shape;Trajectory;control system synthesis;dexterous manipulators;elasticity;flexible manipulators;manipulator dynamics;motion control;muscle;bending motion control;biologically inspired impedance control;control scheme design;dynamic movement;multisegment flexible robot arm;muscle activation;octopus arm;robot dynamic model;stiffness;},
                                                        doi={10.1109/ICRA.2012.6225070},
                                                        ISSN={1050-4729}
                                                        }</pre>
                                                </blockquote>
                                            </div>
                                            
                                            <div class="pubspan">
                                                [<a href="javascript:copy(div2012octopus, abs2012octopus)">abstract</a>]
                                                [<a href="javascript:copy(div2012octopus, bib2010octopus)">bibtex</a>]
                                                [<a href="papers/ICRA12_1385_FI.pdf">Full Paper</a>]
                                            </div>
                                            <div id="div2012octopus" class="pubInfo"></div>
                                        </div></td></tr>
                                <!-- ------------------------------------------------------------------------
                                 2011
                                 --------------------------------------------------------------------- -->
                                <!--li class="year"> 2011 </li-->
                                <!--
                                 2011 Driving
                                 -->
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/Junior.png" width="180">
                                            </td>
                                    <td class="pubitem">
                                        <div class="pubitem">
                                            <a class="pubtitle" href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5940562">Towards fully autonomous driving: Systems and algorithms</a>
                                            <div>
                                                Jesse Levinson, Jake Askeland, Jan Becker, Jennifer Dolson, <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>, Soeren Kammel,
                                                J. Zico Kolter, Dirk Langer, Oliver Pink, Vaughan Pratt, Michael Sokolsky,
                                                Ganymed Stanek, David Stavens, Alex Teichman, Moritz Werling, and Sebastian Thrun
                                            </div>
                                            <div class="pubjournal">
                                                Intelligent Vehicles Symposium (IV), 2011.
                                            </div>
                                            
                                            <div id="abs2011IV" style="display:none">
                                                <blockquote>
                                                    In order to achieve autonomous operation of a vehicle in urban situations with unpredictable traffic, several realtime systems must interoperate, including environment perception, localization, planning, and control. In addition, a robust vehicle platform with appropriate sensors, computational hardware, networking, and software infrastructure is essential. We previously published an overview of Junior, Stanford's entry in the 2007 DARPA Urban Challenge. This race was a closed-course competition which, while historic and inciting much progress in the field, was not fully representative of the situations that exist in the real world. In this paper, we present a summary of our recent research towards the goal of enabling safe and robust autonomous operation in more realistic situations. First, a trio of unsupervised algorithms automatically calibrates our 64-beam rotating LIDAR with accuracy superior to tedious hand measurements. We then generate high-resolution maps of the environment which are subsequently used for online localization with centimeter accuracy. Improved perception and recognition algorithms now enable Junior to track and classify obstacles as cyclists, pedestrians, and vehicles; traffic lights are detected as well. A new planning system uses this incoming data to generate thousands of candidate trajectories per second, choosing the optimal path dynamically. The improved controller continuously selects throttle, brake, and steering actuations that maximize comfort and minimize trajectory error. All of these algorithms work in sun or rain and during the day or night. With these systems operating together, Junior has successfully logged hundreds of miles of autonomous operation in a variety of real-life conditions.
                                                </blockquote>
                                            </div>
                                            
                                            <div id="bib2011IV" style="display:none">
                                                <blockquote>
                                                    <pre>@INPROCEEDINGS{5940562,
                                                        author={Levinson, J. and Askeland, J. and Becker, J. and Dolson, J. and Held, D. and Kammel, S. and Kolter, J.Z. and Langer, D. and Pink, O. and Pratt, V. and Sokolsky, M. and Stanek, G. and Stavens, D. and Teichman, A. and Werling, M. and Thrun, S.},
                                                        booktitle={Intelligent Vehicles Symposium (IV), 2011 IEEE}, title={Towards fully autonomous driving: Systems and algorithms},
                                                        year={2011},
                                                        month={june},
                                                        volume={},
                                                        number={},
                                                        pages={163 -168},
                                                        keywords={Calibration;Laser beams;Planning;Software;Trajectory;Vehicle dynamics;Vehicles;computer vision;mobile robots;remotely operated vehicles;DARPA urban challenge;LIDAR;autonomous driving;closed-course competition;environment perception;obstacle classification;obstacle tracking;online localization;planning system;realtime system;recognition algorithm;robust autonomous operation;robust vehicle platform;software infrastructure;unpredictable traffic;},
                                                        doi={10.1109/IVS.2011.5940562},
                                                        ISSN={1931-0587}}</pre>
                                                </blockquote>
                                            </div>
                                            
                                            <div class="pubspan">
                                                [<a href="javascript:copy(div2011IV, abs2011IV)">abstract</a>]
                                                [<a href="javascript:copy(div2011IV, bib2011IV)">bibtex</a>]
                                            </div>
                                            <div id="div2011IV" class="pubInfo"></div>
                                        </div></td></tr>
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/hovercraft.png" width="180">
                                            </td>
                                    
                                    <td class="pubitem">
                                        <div class="pubitem">
                                            <a class="pubtitle" href="papers/jin+04-acc.pdf">MVWT-II: The Second Generation Caltech Multi-Vehicle Wireless Testbed</a>
                                            <!-- Official link: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1384698 -->
                                            <div class="pubauthor">
                                                Zhipu Jinh,  Stephen Waydo,  Elisabeth B. Wildanger,  Michael Lammers,
                                                Hans Scholze, Peter Foley,
                                                <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>,
                                                Richard M. Murray
                                            </div>
                                            <div class="pubjournal">
                                                American Control Conference (ACC), 2004
                                            </div>
                                            
                                            <div id="abs2004acc" style="display:none">
                                                <blockquote>
                                                    The Caltech Multi-Vehicle Wireless Testbed is an experimental platform for validating theoretical advances in multiple-vehicle coordination and cooperation, real-time networked control system, and distributed computation. This paper describes the design and development of an additional fleet of 12 second-generation vehicles. These vehicles are hovercrafts and designed to have lower mass and friction as well as smaller size than the first generation vehicles. These hovercrafts combined with the outdoor wireless testbed provide a perfect hardware platform for RoboFlag competition.
                                                </blockquote>
                                            </div>
                                            
                                            <div id="bib2004acc" style="display:none">
                                                <blockquote>
                                                    <pre>@INPROCEEDINGS{1384698,
                                                        author={Jin, Z. and Waydo, S. and Wildanger, E.B. and Lammers, M. and Scholze, H. and Foley, P. and Held, D. and Murray, R.M.},
                                                        booktitle={American Control Conference, 2004. Proceedings of the 2004}, title={MVWT-II: the second generation Caltech Multi-Vehicle Wireless Testbed},
                                                        year={2004},
                                                        month={30 2004-july 2},
                                                        volume={6},
                                                        number={},
                                                        pages={5321 -5326 vol.6},
                                                        keywords={design engineering;games of skill;hovercraft;mobile robots;multi-robot systems;real-time systems;Caltech MultiVehicle Wireless Testbed;RoboFlag competition;distributed computation;hardware platform;hovercraft design;multivehicle cooperation;multivehicle coordination;outdoor wireless testbed;real time networked control system;second generation vehicles;},
                                                        doi={},
                                                        ISSN={0743-1619}}</pre>
                                                </blockquote>
                                            </div>
                                            
                                            <div class="pubspan">
                                                [<a href="javascript:copy(div2004acc, abs2004acc)">abstract</a>]
                                                [<a href="javascript:copy(div2004acc, bib2004acc)">bibtex</a>]
                                            </div>
                                            <div id="div2004acc" class="pubInfo"></div>
                                        </div></td></tr>
                                
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/tactors.png" width="180">
                                            <!--img src="DavidHeld_files/tactors.png" width="180"-->
                                            </td>
                                    
                                    <td class="pubitem">
                                        <div class="pubitem">
                                            <a class="pubtitle" href="papers/78658.pdf">Surface waves and spatial localization in vibrotactile displays</a>
                                            <!-- Official link: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5444673 -->
                                            <div>
                                                <a class="pubauthor" href="http://meche.mit.edu/people/index.html?id=128">Lynette Jones</a>,
                                                <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>,
                                                <a class="pubauthor" href="http://meche.mit.edu/people/index.html?id=46">Ian Hunter</a>
                                            </div>
                                            <div class="pubjournal">
                                                Haptics Symposium, 2010
                                            </div>
                                            
                                            <div id="abs2010vibrations" style="display:none">
                                                <blockquote>
                                                    The locus of vibrotactile stimulation is often used as an encoding cue in tactile displays developed for spatial orientation and navigation. However, the ability to localize the site of stimulation varies as a function of the number and configuration of the vibrating motors (tactors) in the display. As the inter-tactor distance decreases it has been found that the ability to localize a point of stimulation diminishes. One factor that may limit tactile localization is the surface wave elicited by vibration that propagates across the skin at a velocity that depends on the frequency of vibration and the viscoelastic properties of the skin. A material that simulates the stress-strain characteristics of human skin was used to measure the characteristics of surface waves during vibrotactile stimulation. Accelerometers glued to the simulated skin at fixed distances from the activated tactors were used to measure the amplitude of the acceleration as a function of distance as well as the propagation velocity. It was determined that at a distance of 60 mm from the site of activation, the surface wave was on average attenuated to less than 1 m/s^2. This suggests that for this type of tactor an inter-tactor distance of at least 60 mm would be optimal for a display in which the locus of stimulation is used as an encoding variable. It seems that much of the difficulty encountered in identifying the locus of a vibrotactile stimulus in multi-tactor displays may result from small intertactor distances.
                                                </blockquote>
                                            </div>
                                            
                                            <div id="bib2010vibrations" style="display:none">
                                                <blockquote>
                                                    <pre>@INPROCEEDINGS{5444673,
                                                        author={Jones, L.A. and Held, D. and Hunter, I.},
                                                        booktitle={Haptics Symposium, 2010 IEEE}, title={Surface waves and spatial localization in vibrotactile displays},
                                                        year={2010},
                                                        month={march},
                                                        volume={},
                                                        number={},
                                                        pages={91 -94},
                                                        keywords={Accelerometers;Displays;Elasticity;Encoding;Frequency;Humans;Navigation;Skin;Surface waves;Viscosity;accelerometers;haptic interfaces;accelerometers;distance 60 mm;inter-tactor distance;spatial localization;stimulation locus;stress-strain characteristics;surface waves;vibrating motors;vibrotactile displays;vibrotactile stimulation;localization;tactile display;tactons;torso;touch;},
                                                        doi={10.1109/HAPTIC.2010.5444673},
                                                        ISSN={}}</pre>
                                                </blockquote>
                                            </div>
                                            
                                            <div class="pubspan">
                                                [<a href="javascript:copy(div2010vibrations, abs2010vibrations)">abstract</a>]
                                                [<a href="javascript:copy(div2010vibrations, bib2010vibrations)">bibtex</a>]
                                            </div>
                                            <div id="div2010vibrations" class="pubInfo"></div>
                                        </div></td></tr>
                                
                                <tr>
                                    <td class="pubimg">
                                        <img src="DavidHeld_files/tactors1.png" width="180">
                                            <!--img src="DavidHeld_files/tactors1.png" width="180"-->
                                            </td>
                                    
                                    <td class="pubitem">
                                        <div class="pubitem">
                                            <a class="pubtitle" href="http://computingengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1401367">Characterization of Tactors Used in Vibrotactile Displays</a>
                                            <div>
                                                <a class="pubauthor" href="http://meche.mit.edu/people/index.html?id=128">Lynette Jones</a>,
                                                <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>
                                            </div>
                                            <div class="pubjournal">
                                                Journal of Computing and Information Science in Engineering, 2008
                                            </div>
                                            
                                            <div id="abs2008jcis" style="display:none">
                                                <blockquote>
                                                    A series of experiments was conducted to evaluate the operating characteristics of small DC motors that are often in tactile displays. The results indicated that these motors are reliable in terms of their frequency and amplitude of oscillation, but that the frequency varies across motors. A simulated skin material was developed to provide a substrate for evaluating the performance of the motors. There was a marked attenuation in frequency when the tactors were on this material and the surface waves could be detected 60 mm from the site of activation. These findings suggest that the spacing between tactors should be at least 60-80 mm if tactile cues are used to locate events in the environment.
                                                </blockquote>
                                            </div>
                                            
                                            <div id="bib2008jcis" style="display:none">
                                                <blockquote>
                                                    <pre>@article{jones_held:044501,
                                                        author = {Lynette A. Jones and David A. Held},
                                                        collaboration = {},
                                                        title = {Characterization of Tactors Used in Vibrotactile Displays},
                                                        publisher = {ASME},
                                                        year = {2008},
                                                        journal = {Journal of Computing and Information Science in Engineering},
                                                        volume = {8},
                                                        number = {4},
                                                        eid = {044501},
                                                        numpages = {5},
                                                        pages = {044501},
                                                        keywords = {DC motors; display devices; haptic interfaces},
                                                        url = {http://link.aip.org/link/?CIS/8/044501/1},
                                                        doi = {10.1115/1.2988384}
                                                        }</pre>
                                                </blockquote>
                                            </div>
                                            
                                            <div class="pubspan">
                                                [<a href="javascript:copy(div2008jcis, abs2008jcis)">abstract</a>]
                                                [<a href="javascript:copy(div2008jcis, bib2008jcis)">bibtex</a>]
                                            </div>
                                            <div id="div2008jcis" class="pubInfo"></div>
                                        </div></td></tr>
                                
                                <!-- ------------------------------------------------------------------------
                                 YEAR
                                 --------------------------------------------------------------------- -->
                                <!--li class="year"> YEAR </li-->
                                
                                <!--
                                 ENTRY NAME
                                 -->
                                <!--li class="pubitem">
                                 <a class="pubtitle" href="">TITLE</a>
                                 <div>
                                 <a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>,
                                 OTHER AUTHORS
                                 </div>
                                 <div class="pubjournal">
                                 CONFERENCE NAME (ABBREVIATION). YEAR
                                 </div>
                                 
                                 <div id="absNEW" style="display:none">
                                 <blockquote>
                                 ABSTRACT
                                 </blockquote>
                                 </div>
                                 
                                 <div id="bibNEW" style="display:none">
                                 <blockquote>
                                 <pre> BIBTEX</pre>
                                 </blockquote>
                                 </div>
                                 
                                 <div class="pubspan">
                                 [<a href="javascript:copy(divNEW, absNEW)">abstract</a>]
                                 [<a href="javascript:copy(divNEW, bibNEW)">bibtex</a>]
                                 </div>
                                 <div id="divNEW" class="pubInfo"></div>
                                 </li-->
                                <!--/ul-->
                                <!--/tr-->
                            </table>
                            
                            <!--div class="heading"> Pre-Prints </div>
                            
                            
                            <table cellpadding="10" width="100%">
                                
                            </table-->
                            <!-- Add this to a new tab later
                            <div class="heading"> Advice </div>
                            
                            
                            
                             table cellpadding="10" width="100%">
                                
                                <tr>
                                    <td>
                                        I gave a talk on <a href="https://www.youtube.com/watch?v=CDXZXljc2Ic">"Demystifying the Post-doc - Why, Where, and How?"</a>  If you are considering doing a post-doc, the advice in this talk might be helpful.
                                        
                                        <!--In this talk, I describe the ins and outs of getting a job as a post-doctoral researcher ("doing a post-doc").  I first describe a number of reasons why one might want to do a post-doc, as well as a few reasons not to do one.  I next describe how to choose where one should apply for a post-doc.  Next, I describe the process of how to get a job in a dream post-doc position.  I also describe how to choose among multiple post-doc offers.  Last, I will describe how to be "successful" as a post-doc, i.e. how to make the most of your time as a post-doc and prepare for your future career.  You can watch this video-->
                                        <!--
                                    </td>
                                </tr>
                            </table>-->


                            <p>
                            
                            <!--END CONTENT-->
                            
                            
                            
                            <div style="clear: both;"></div>
                            </div>
                <!--div class="figure">
                 <img src="./DavidHeld_files/DavidHeld.jpg" width="150"><br><br>
                 <img src="./DavidHeld_files/email.png" width="210"><br>
                 </div-->
            </div>
        </div>
    </div>

        </td>
        <td valign="top" align="center" style='padding:0px 0px 0px 100px'>
            <!--div class="figure"-->
            <img src="./DavidHeld_files/DavidHeld.jpg" width="150"><br><br>
            <img src="./DavidHeld_files/email.png" width="210"><br><br>
                Elliot Dunlap Smith Hall (EDSH), Room 213
                    <!--/div-->
        </td>
        </tr>
        </table>
        
        
    </div></body></html>
