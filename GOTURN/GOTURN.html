<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<link rel="stylesheet" type="text/css" href="../DavidHeld_files/project.css">


<title>Probabilistic Framework for Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues</title>
</head>

<body>

<!--body style='margin-top:0;margin-left:0;margin-right:0;'-->
<div id="outer">

<div class="bottom">

<!--div style="float:left; width:62%"></div-->

<div class="left">
<img src="../DavidHeld_files/bg-grey.png" id="gradient">
</div>

<div class="right">

<h1><center>Learning to Track at 100 FPS with Deep Regression Networks</center></h1>
<center>
<font size="5"><!-- face="Arial"><i>-->
<a class="pubauthor" href="http://stanford.edu/~davheld">David Held</a>,
<a class="pubauthor" href="http://robots.stanford.edu/">Sebastian Thrun,</a>
<a class="pubauthor" href="http://cvgl.stanford.edu/silvio/">Silvio Savarese</a>
</i>
</font>
</center>

<h2>Abstract</h2>
Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance.  Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training.  We propose a method for using neural networks to track generic objects in a way that allows them to improve performance by training on labeled videos.  Previous attempts to use neural networks for tracking are very slow to run and not practical for real-time applications.  In contrast,
our tracker uses a simple feed-forward network with no online training required, allowing our tracker to run at 100 fps during test time.  Our tracker trains from both labeled video as well as a large collection of images, which helps prevent overfitting.  The tracker learns generic object motion and can be used to track novel objects that do not appear in the training set.  We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance.  Our network learns to track generic objects in real-time as they move throughout the world.
<br><br>
<center><img width=80% src="pull7f-web_e2.png"></center>



<h2>Publications</h2>
<b>Learning to Track at 100 FPS with Deep Regression Networks</b><br>
<a class="pubauthor">David Held, Sebastian Thrun, Silvio Savarese</a>
<div class="pubjournal">European Conference on Computer Vision (ECCV), 2016 (In press)</div>
<!--a href="RSS_2016_Final_Version_28.pdf">[Full Paper]</a>
<a href="supplement.pdf">[Supplement]</a>-->

<h2>Code</h2>
The C++ code for the neural-network tracker is available on github <a href="https://github.com/davheld/GOTURN">here</a>
<p>
If you have any further questions about the tracker, please email me at davheld -at- cs -dot- stanford -dot- edu.


<h2> Videos </h2>
The performance of GOTURN can be seen in this video:
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/kMhwXnLgT_I" frameborder="0" allowfullscreen></iframe>
</center>
<h2>Bibtex</h2>
<pre>
@inproceedings{held2016learning,
title={Learning to Track at 100 FPS with Deep Regression Networks},
author={Held, David and Thrun, Sebastian and Savarese, Silvio},
booktitle = {European Conference Computer Vision (ECCV)},
year      = {2016}
}
</pre>

<h2>Questions?</h2>
If you have any further questions about our method, please email me at davheld -at- cs -dot- stanford -dot- edu.
<br /><br /><br /><br /><br /><br /><br /><br />
</div></div></div>
</body></html>
